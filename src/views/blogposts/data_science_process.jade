extends ../_layout--detail
block content
  article.article
    div.img--is-in-background
      img(class="img--is-in-background__img" src="/assets/images/welcome.jpg")
      div.img--is-in-background__content
        h1 The Data Science Process
        //-span.heading-sub My top five moments
    header.article__header
      img(class="img--is-rounded" src="/assets/images/luki.jpg")
      p.meta
        a(class="link--has-no-underline" href="" rel="author") Lukas Hodel
        |  wrote this 
        time(datetime="2015-11-28") one day ago
      a(class="link-icon icon-twitter" href="https://twitter.com/agento")
    hr
    p.
      Every professional field comes with unique needs and its special
      gotchas to be aware of. As people working in their environments, they thend
      to learn from errors and come up with solid paths to hold on. You can call
      them best practices, red line or formal "process models" as you like it.
      <br><br>
      Also in data science exists a way to follow to extend your chance to
      be successfull. With this post, I try to show the one I followed through my
      first data science prject at HitFox Group.
    h2 A few easy steps
    p.
      <strong>Defining a baseline</strong> is the first step which has to be done in every
      data science project. The baseline translates the outcome of the model 
      to a real-live cost dimension. It also is used to measure the cost 
      savings by the intelligent model accoring a basic stupid one.
      <strong>When you are not able to do define a proper baseline, maybe machine-learning is not helping you out at all</strong>. 
      Things to consider while defining the 
      baseline are:
    ul
      li What do we want to automate/optimize?
      li How do we do it today?
      li. 
        How can we measure the process, so that we can validate a potential 
        model, in a realistic environment?
    blockquote(cite="" class="blockquote")
      p(class="blockquote__p").  
        We humans, train the machines of the future with our 
        intelligent behaviour today!
    p.
      <strong>Getting the Data</strong> is the obvious, most important part. Haveing good
      labeled quality data is even more important than being the best
      data scientist in the world, knowing the best algorithms. That's one reason
      why many big companies are giving you free products payed by your 
      behaviour data. We humans, train the machines of the future by our 
      intelligent behaviour today!
    ul
      li What kind of Data is important for the use-case?
      li Where do we have the relevant data from?
      li Is the data labeled properly?
      li Is there <strong>enough</strong> data to be generalized? 
      li. 
        Are we allowed to own the data? Check the german right for privacy!
    p.
      <strong>Splitting into train, validation and test sets</strong>. When
      training a model there has to be data left to prove the results with 
      not touched data. In this step it is important to check, that the 
      datasets are good balanced. A good way to do is by splitting the sets
      in a random way. My prefered way is to take the older data for training
      and the newer for test and validation. This has a kind of real-world
      randomness in it.
    p.
      <strong>Preprocess the data</strong>. The raw data often is in a very
      chaotic state. Depending on the need, with preprocessing you can shape
      the data to make it more meaningfull. In natural language processing
      The text is cleand from meta characters and stopwords which have no
      contextual meaning. In the preprocessing step the data is also tranfered
      from a arbitrary form (like text) to a machine processable mathematical
      form (vectors/matrices). This transformation is often combined with the 
      next step.
    p.
      <strong>Feature engeneering</strong> is after "getting the data" the next
      underrated task. With feature engeneering we try to find the good 
      properties and in the data which makes the most sense for the problem to 
      solve.
      You have to have a deep understanding of the problem and its context for
      chosing, finding and producing good features. Here often it is more
      valuable to partner with Phsychologists and Sociologists than the good
      programmer next door. They are doing data science since years, when not
      coupled with fancy machine-learning.
    ul
      li What kind of data do I have?
      li What information makes sense for the question to solve?
      li Is the hair color relevant to predict the fashion style?
      li is the hair color relevant to predict liquidity?
      li Can I <strong>combine features and external data sources</strong> to get new ones?
    p.
      <strong>Design and train the model</strong>. This is the step where the 
      algorithms come to play, and in my opinion the "hype" in data science
      take place. When the data is in a good shape it is time to 
      choose/combine the algorithms and design a model. The model
      gets trained with the training data and afterwards validated with the 
      validation data. The validation data is also used to finetune the 
      parameters for the model. 
    p.
      <strong>Test it with new production data</strong> before deploy it to 
      production. It is strongly recommended to test it with new data,
      you never have seen before. When it still performs good the model is mature
      to <strong>influence / manipulate our behaviour</strong>.
    
      
    
      
